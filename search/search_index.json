{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reductionist \u00b6 Reductionist is a web service that provides an API to perform reductions on numerical data stored in an S3-compatible object store. By implementing these reductions in the storage system the volume of data that needs to be transferred to the end user is vastly reduced, leading to faster computations. The work is funded by the ExCALIBUR project and is done in collaboration with the University of Reading . This site provides documentation for the Reductionist application. Documentation for the source code is available on docs.rs . This is a performant implementation of the Active Object Storage server. The original Python functional prototype is available here . Note: The original S3 Active Storage project was renamed to Reductionist, to avoid confusion due to overuse of the term Active Storage. Features \u00b6 Reductionist provides the following features: HTTP(S) API with JSON request data Access to data stored in S3-compatible storage On-disk caching of downloaded data to speed up repeat data requests Basic numerical operations on multi-dimensional arrays (count, min, max, select, sum) Perform calculations on a selection/slice of an array Perform calculations allowing for missing data Compressed data (GZip, Zlib) Filtered data (byte shuffle) Data with non-native byte order (endianness) Server resource (CPU, memory, files) management Prometheus metrics Tracing with an option to send data to Jaeger Ansible-based containerised deployment Related projects \u00b6 PyActiveStorage is a Python library which performs reductions on numerical data in data sources such as netCDF4. It has support for delegating computation to Reductionist when the data is stored in an S3-compatible object store.","title":"Home"},{"location":"#reductionist","text":"Reductionist is a web service that provides an API to perform reductions on numerical data stored in an S3-compatible object store. By implementing these reductions in the storage system the volume of data that needs to be transferred to the end user is vastly reduced, leading to faster computations. The work is funded by the ExCALIBUR project and is done in collaboration with the University of Reading . This site provides documentation for the Reductionist application. Documentation for the source code is available on docs.rs . This is a performant implementation of the Active Object Storage server. The original Python functional prototype is available here . Note: The original S3 Active Storage project was renamed to Reductionist, to avoid confusion due to overuse of the term Active Storage.","title":"Reductionist"},{"location":"#features","text":"Reductionist provides the following features: HTTP(S) API with JSON request data Access to data stored in S3-compatible storage On-disk caching of downloaded data to speed up repeat data requests Basic numerical operations on multi-dimensional arrays (count, min, max, select, sum) Perform calculations on a selection/slice of an array Perform calculations allowing for missing data Compressed data (GZip, Zlib) Filtered data (byte shuffle) Data with non-native byte order (endianness) Server resource (CPU, memory, files) management Prometheus metrics Tracing with an option to send data to Jaeger Ansible-based containerised deployment","title":"Features"},{"location":"#related-projects","text":"PyActiveStorage is a Python library which performs reductions on numerical data in data sources such as netCDF4. It has support for delegating computation to Reductionist when the data is stored in an S3-compatible object store.","title":"Related projects"},{"location":"api/","text":"API \u00b6 The Reductionist API accepts HTTP POST requests to /v1/{operation} , where {operation} is the name of the operation to perform, one of count , min , max , sum or select . The request body should be a JSON object of the form: { // The URL for the S3 source // - required \"source\": \"https://s3.example.com/, // The name of the S3 bucket // - required \"bucket\": \"my-bucket\", // The path to the object within the bucket // - required \"object\": \"path/to/object\", // The data type to use when interpreting binary data // - required \"dtype\": \"int32|int64|uint32|uint64|float32|float64\", // The byte order (endianness) of the data // - optional, defaults to native byte order of Reductionist server \"byte_order\": \"big|little\", // The offset in bytes to use when reading data // - optional, defaults to zero \"offset\": 0, // The number of bytes to read // - optional, defaults to the size of the entire object \"size\": 128, // The shape of the data (i.e. the size of each dimension) // - optional, defaults to a simple 1D array \"shape\": [20, 5], // The axis or axes over which to perform the reduction operation // - optional, can be either a single axis or list of axes, defaults // to a reduction over all axes \"axis\": 0, // Indicates whether the data is in C order (row major) // or Fortran order (column major, indicated by 'F') // - optional, defaults to 'C' \"order\": \"C|F\", // An array of [start, end, stride] tuples indicating the data to be operated on // (if given, you must supply one tuple per element of \"shape\") // - optional, defaults to the whole array \"selection\": [ [0, 19, 2], [1, 3, 1] ], // Algorithm used to compress the data // - optional, defaults to no compression \"compression\": {\"id\": \"gzip|zlib\"}, // List of algorithms used to filter the data // - optional, defaults to no filters \"filters\": [{\"id\": \"shuffle\", \"element_size\": 4}], // Missing data description // - optional, defaults to no missing data // - exactly one of the keys below should be specified // - the values should match the data type (dtype) \"missing\": { \"missing_value\": 42, \"missing_values\": [42, -42], \"valid_min\": 42, \"valid_max\": 42, \"valid_range\": [-42, 42], } } Request authentication is implemented using Basic Auth with the username and password consisting of your S3 Access Key ID and Secret Access Key, respectively. Unauthenticated access to S3 is possible by omitting the basic auth header. On success, all operations return HTTP 200 OK with the response using the same datatype as specified in the request except for count which always returns the result as int64 . The server returns the following headers with the HTTP response: x-activestorage-dtype : The data type of the data in the response payload. One of int32 , int64 , uint32 , uint64 , float32 or float64 . x-activestorage-byte-order : The byte order of the data in the response payload. Either big or little . x-activestorage-shape : A JSON-encoded list of numbers describing the shape of the data in the response payload. May be an empty list for a scalar result. x-activestorage-count : The number of non-missing array elements operated on while performing the requested reduction. This header is useful, for example, to calculate the mean over multiple requests where the number of items operated on may differ between chunks. On error, an HTTP 4XX (client) or 5XX (server) response code will be returned, with the response body being a JSON object of the following format: { \"error\": { // Main error message \"message\": \"error receiving object from S3 storage\", // Optional list of lower-level errors, with the root cause last \"caused_by\": [ \"IO error\", \"unexpected end of file\" ] } } The scripts/client.py provides an example Python client and Command Line Interface (CLI).","title":"API"},{"location":"api/#api","text":"The Reductionist API accepts HTTP POST requests to /v1/{operation} , where {operation} is the name of the operation to perform, one of count , min , max , sum or select . The request body should be a JSON object of the form: { // The URL for the S3 source // - required \"source\": \"https://s3.example.com/, // The name of the S3 bucket // - required \"bucket\": \"my-bucket\", // The path to the object within the bucket // - required \"object\": \"path/to/object\", // The data type to use when interpreting binary data // - required \"dtype\": \"int32|int64|uint32|uint64|float32|float64\", // The byte order (endianness) of the data // - optional, defaults to native byte order of Reductionist server \"byte_order\": \"big|little\", // The offset in bytes to use when reading data // - optional, defaults to zero \"offset\": 0, // The number of bytes to read // - optional, defaults to the size of the entire object \"size\": 128, // The shape of the data (i.e. the size of each dimension) // - optional, defaults to a simple 1D array \"shape\": [20, 5], // The axis or axes over which to perform the reduction operation // - optional, can be either a single axis or list of axes, defaults // to a reduction over all axes \"axis\": 0, // Indicates whether the data is in C order (row major) // or Fortran order (column major, indicated by 'F') // - optional, defaults to 'C' \"order\": \"C|F\", // An array of [start, end, stride] tuples indicating the data to be operated on // (if given, you must supply one tuple per element of \"shape\") // - optional, defaults to the whole array \"selection\": [ [0, 19, 2], [1, 3, 1] ], // Algorithm used to compress the data // - optional, defaults to no compression \"compression\": {\"id\": \"gzip|zlib\"}, // List of algorithms used to filter the data // - optional, defaults to no filters \"filters\": [{\"id\": \"shuffle\", \"element_size\": 4}], // Missing data description // - optional, defaults to no missing data // - exactly one of the keys below should be specified // - the values should match the data type (dtype) \"missing\": { \"missing_value\": 42, \"missing_values\": [42, -42], \"valid_min\": 42, \"valid_max\": 42, \"valid_range\": [-42, 42], } } Request authentication is implemented using Basic Auth with the username and password consisting of your S3 Access Key ID and Secret Access Key, respectively. Unauthenticated access to S3 is possible by omitting the basic auth header. On success, all operations return HTTP 200 OK with the response using the same datatype as specified in the request except for count which always returns the result as int64 . The server returns the following headers with the HTTP response: x-activestorage-dtype : The data type of the data in the response payload. One of int32 , int64 , uint32 , uint64 , float32 or float64 . x-activestorage-byte-order : The byte order of the data in the response payload. Either big or little . x-activestorage-shape : A JSON-encoded list of numbers describing the shape of the data in the response payload. May be an empty list for a scalar result. x-activestorage-count : The number of non-missing array elements operated on while performing the requested reduction. This header is useful, for example, to calculate the mean over multiple requests where the number of items operated on may differ between chunks. On error, an HTTP 4XX (client) or 5XX (server) response code will be returned, with the response body being a JSON object of the following format: { \"error\": { // Main error message \"message\": \"error receiving object from S3 storage\", // Optional list of lower-level errors, with the root cause last \"caused_by\": [ \"IO error\", \"unexpected end of file\" ] } } The scripts/client.py provides an example Python client and Command Line Interface (CLI).","title":"API"},{"location":"architecture/","text":"Architecture and implementation \u00b6 Reductionist is written in Rust , a language that is rapidly gaining popularity for a variety of use cases. It provides high level abstractions with low runtime overhead, a modern toolchain, and has a unique approach that provides safe automatic memory management without garbage collection. While the Rust standard library is not as comprehensive as some other \"batteries included\" languages, the crates.io ecosystem is relatively mature and provides a number of de-facto standard libraries. Reductionist is built on top of a number of popular open source components. A few properties make it relatively easy to build a conceptual mental model of how Reductionist works. All operations share the same request processing pipeline. The request processing pipeline for each request is a fairly linear sequence of steps. There is no persistent state. The only external service that is interacted with is an S3-compatible object store. The more challenging aspects of the system are the lower level details of asynchronous programming, memory management, the Rust type system and working with multi-dimensional arrays. A diagram of the request processing pipeline is shown in Figure 1. Figure 1: Request processing pipeline flow diagram The \"Perform numerical operation\" step depends on the type of numerical operation being performed. A diagram of this step for the sum operation is shown in Figure 2. Figure 2: Sum operation flow diagram Axum web server \u00b6 Axum is an asynchronous web framework that performs well in various benchmarks and is built on top of various popular components, including the hyper HTTP library. It integrates well with Tokio , the most popular asynchronous Rust runtime, and allows us to easily define an API route for each operation. Extractors make it easy to consume data from the request in a type-safe way. The operation request handler is the operation_handler function in src/app.rs . API request data \u00b6 The JSON request data is deserialised into the RequestData struct defined in src/models.rs using the serde library. Serde handles conversion errors at the type level, while further validation of request data invariants is performed using the validator crate. S3 object download \u00b6 Object data is downloaded from the object store using the AWS SDK . The S3Client struct in src/s3_client.rs provides a simplified wrapper around the AWS SDK. Typically we will be operating on a \"storage chunk\", a hyperslab within the larger dataset that the object contains. In this case a byte range is specified in the S3 GetObject request to avoid downloading the whole object. The AWS SDK is asynchronous and does provide a streaming response, however we read the whole storage chunk into memory to simplify later stages of the pipeline. Storage chunks are expected to be small enough (O(MiB)) that this should not be a problem. Construction of aws_sdk_s3::Client structs is a relatively slow task. A key performance improvement involves the use of a shared client object for each combination of object store URL and credentials. This is implemented using the S3ClientMap in src/s3_client.rs and benchmarked in benches/s3_client.rs . Downloaded storage chunk data is returned to the request handler as a Bytes object, which is a wrapper around a u8 (byte) array. S3 object caching \u00b6 A cache can be optionally enabled to store downloaded S3 objects to disk, this allows the Reductionist to repeat operations on already downloaded data objects utilising faster disk I/O over network I/O. Authentication is passed through to the S3 object store and access to cached data by users other than the original requestor is allowed if S3 authentication permits. Authentication can be optionally disabled for further cache speedup in trusted environments. A Tokio MPSC channel bridges write access between the requests of the asynchronous Axum web framework and synchronous writes to the disk cache; this allows requests to the Reductionist to continue unblocked along their operation pipeline whilst being queued for cache storage. The disk cache can be managed overall by size and by time to live (TTL) on individual data objects with automatic pruning removing expired objects. Cache state is maintained on-disk allowing the cache to be reused across restarts of the Reductionist server. Filters and compression \u00b6 When a variable in a netCDF, HDF5 or Zarr dataset is created, it may be compressed to reduce storage requirements. Additionally, prior to compression one or more filters may be applied to the data with the aim of increasing the compression ratio. When consuming such data, Reductionist needs to reverse any compression and filters applied. The filter pipeline is implemented in src/filter_pipeline.rs . First, if a compression algorithm is specified in the request data, the storage chunk is decompressed using the same algorithm. Currently the Gzip and Zlib algorithms are supported using the flate2 and zune-inflate libraries respectively. This mix of libraries was chosen based on performance benchmarks in benches/compression.rs . Compression is implemented in src/compression.rs . Next, if any filters are specified in the request data, they are decoded in reverse order. Currently the byte shuffle filter is supported. This filter reorders the data to place the Nth bytes of each data value together, with the aim of grouping leading zeroes. The shuffle filter is implemented in src/filters/shuffle.rs , and has several optimisations including loop unrolling that were benchmarked using benches/shuffle.rs . The Operation trait \u00b6 Here the implementation becomes specific to the requested operation (min, max, etc.). This is achieved using the Operation trait defined in src/operation.rs . /// Trait for active storage operations. /// /// This forms the contract between the API layer and operations. pub trait Operation { /// Execute the operation. /// /// Returns a [models::Response](crate::models::Response) object with response data. /// /// # Arguments /// /// * `request_data`: RequestData object for the request /// * `data`: [`Vec<u8>`] containing data to operate on. fn execute ( request_data : & models :: RequestData , data : Vec < u8 > , ) -> Result < models :: Response , ActiveStorageError > ; } This interface accepts the request data and a byte array containing the storage chunk data in its original byte order. On success, it returns a Response struct which contains a byte array of the response data as well as the data type, shape and a count of non-missing elements in the array. A second NumOperation trait with an execute_t method handles the dynamic dispatch between the runtime data type in the request data and the generic implementation for that type. Operations \u00b6 Each operation is implemented by a struct that implements the NumOperation trait. For example, the sum operation is implemented by the Sum struct in src/operations.rs . The Sum struct's execute_t method does the following: Zero copy conversion of the byte array to a multi-dimensional ndarray::ArrayView object of the data type, shape and byte order specified in the request data If a selection was specified in the request data, create a sliced ndarray::ArrayView onto the original array view Checks whether the reduction should be performed over all or only a subset of the sliced data's axes Performs a fold over each of the requested axes to calculate the required reduction while ignoring any specified missing data Convert the sum to a byte array and return with the element count The procedure for other operations varies slightly but generally follows the same pattern. Error handling \u00b6 The ActiveStorageError enum in src/error.rs describes the various errors that may be returned by the Reductionist API, as well as how to format them for the JSON error response body. Low-level errors are converted to higher-level errors and ultimately wrapped by ActiveStorageError . This is a common pattern in Rust and allows us to describe all of the errors that a function or application may return. Configuration \u00b6 Reductionist configuration is implemented in src/cli.rs using the clap library, and accepts command line arguments and environment variables. Resource management \u00b6 Reductionist supports optional restriction of resource usage. This is implemented in src/resource_manager.rs using Tokio Semaphores . This allows Reductionist to limit the quantity of various resources used at any time: S3 connections memory used for numeric data (this is more of a rough guide than a perfect limit) threads used for CPU-bound work CPU-bound work \u00b6 There is particular friction between the asynchronous and synchronous types of work in the system. Axum and Tokio very efficiently handle the asynchronous aspects such as the HTTP server and S3 object download. The other work such as decompression, filtering and numerical operations are more CPU-bound, and can easily block the Tokio runtime from efficiently handling asynchronous tasks. Two alternative methods were developed to alleviate this issue. The resource manager can limit the number of threads used for CPU-bound work, by default leaving one CPU core free for handling asynchronous tasks. Integration with Rayon , a library that provides a thread pool. Limited benchmarking was done to compare the two approaches, however the first appeared to have lower overhead. The second approach may leave the server more responsive if more CPU-heavy operations are used in future. Monitoring \u00b6 Prometheus metrics are implemented in src/metrics.rs and are exposed by the Reductionist API under the /metrics path. These include: incoming requests (counter) outgoing response (counter) response time (histogram) Tracing and profiling \u00b6 Reductionist integrates with Jaeger, a distributed tracing platform. Various sections of the request processing pipeline are instrumented with spans, making it easy to visualise the relative durations in the Jaeger UI. Testing with a sum over some CMIP6 temperature data, this showed that in terms of wall clock time, the S3 storage chunk download takes the majority of the time, followed by decompression, byte shuffle, and finally the actual numerical operation. Flame graphs created using flamegraph-rs were useful to visualise which parts of the code consume the most CPU cycles. This was useful to determine where to focus performance improvements, and showed that decompression is the most CPU-heavy task.","title":"Architecture"},{"location":"architecture/#architecture-and-implementation","text":"Reductionist is written in Rust , a language that is rapidly gaining popularity for a variety of use cases. It provides high level abstractions with low runtime overhead, a modern toolchain, and has a unique approach that provides safe automatic memory management without garbage collection. While the Rust standard library is not as comprehensive as some other \"batteries included\" languages, the crates.io ecosystem is relatively mature and provides a number of de-facto standard libraries. Reductionist is built on top of a number of popular open source components. A few properties make it relatively easy to build a conceptual mental model of how Reductionist works. All operations share the same request processing pipeline. The request processing pipeline for each request is a fairly linear sequence of steps. There is no persistent state. The only external service that is interacted with is an S3-compatible object store. The more challenging aspects of the system are the lower level details of asynchronous programming, memory management, the Rust type system and working with multi-dimensional arrays. A diagram of the request processing pipeline is shown in Figure 1. Figure 1: Request processing pipeline flow diagram The \"Perform numerical operation\" step depends on the type of numerical operation being performed. A diagram of this step for the sum operation is shown in Figure 2. Figure 2: Sum operation flow diagram","title":"Architecture and implementation"},{"location":"architecture/#axum-web-server","text":"Axum is an asynchronous web framework that performs well in various benchmarks and is built on top of various popular components, including the hyper HTTP library. It integrates well with Tokio , the most popular asynchronous Rust runtime, and allows us to easily define an API route for each operation. Extractors make it easy to consume data from the request in a type-safe way. The operation request handler is the operation_handler function in src/app.rs .","title":"Axum web server"},{"location":"architecture/#api-request-data","text":"The JSON request data is deserialised into the RequestData struct defined in src/models.rs using the serde library. Serde handles conversion errors at the type level, while further validation of request data invariants is performed using the validator crate.","title":"API request data"},{"location":"architecture/#s3-object-download","text":"Object data is downloaded from the object store using the AWS SDK . The S3Client struct in src/s3_client.rs provides a simplified wrapper around the AWS SDK. Typically we will be operating on a \"storage chunk\", a hyperslab within the larger dataset that the object contains. In this case a byte range is specified in the S3 GetObject request to avoid downloading the whole object. The AWS SDK is asynchronous and does provide a streaming response, however we read the whole storage chunk into memory to simplify later stages of the pipeline. Storage chunks are expected to be small enough (O(MiB)) that this should not be a problem. Construction of aws_sdk_s3::Client structs is a relatively slow task. A key performance improvement involves the use of a shared client object for each combination of object store URL and credentials. This is implemented using the S3ClientMap in src/s3_client.rs and benchmarked in benches/s3_client.rs . Downloaded storage chunk data is returned to the request handler as a Bytes object, which is a wrapper around a u8 (byte) array.","title":"S3 object download"},{"location":"architecture/#s3-object-caching","text":"A cache can be optionally enabled to store downloaded S3 objects to disk, this allows the Reductionist to repeat operations on already downloaded data objects utilising faster disk I/O over network I/O. Authentication is passed through to the S3 object store and access to cached data by users other than the original requestor is allowed if S3 authentication permits. Authentication can be optionally disabled for further cache speedup in trusted environments. A Tokio MPSC channel bridges write access between the requests of the asynchronous Axum web framework and synchronous writes to the disk cache; this allows requests to the Reductionist to continue unblocked along their operation pipeline whilst being queued for cache storage. The disk cache can be managed overall by size and by time to live (TTL) on individual data objects with automatic pruning removing expired objects. Cache state is maintained on-disk allowing the cache to be reused across restarts of the Reductionist server.","title":"S3 object caching"},{"location":"architecture/#filters-and-compression","text":"When a variable in a netCDF, HDF5 or Zarr dataset is created, it may be compressed to reduce storage requirements. Additionally, prior to compression one or more filters may be applied to the data with the aim of increasing the compression ratio. When consuming such data, Reductionist needs to reverse any compression and filters applied. The filter pipeline is implemented in src/filter_pipeline.rs . First, if a compression algorithm is specified in the request data, the storage chunk is decompressed using the same algorithm. Currently the Gzip and Zlib algorithms are supported using the flate2 and zune-inflate libraries respectively. This mix of libraries was chosen based on performance benchmarks in benches/compression.rs . Compression is implemented in src/compression.rs . Next, if any filters are specified in the request data, they are decoded in reverse order. Currently the byte shuffle filter is supported. This filter reorders the data to place the Nth bytes of each data value together, with the aim of grouping leading zeroes. The shuffle filter is implemented in src/filters/shuffle.rs , and has several optimisations including loop unrolling that were benchmarked using benches/shuffle.rs .","title":"Filters and compression"},{"location":"architecture/#the-operation-trait","text":"Here the implementation becomes specific to the requested operation (min, max, etc.). This is achieved using the Operation trait defined in src/operation.rs . /// Trait for active storage operations. /// /// This forms the contract between the API layer and operations. pub trait Operation { /// Execute the operation. /// /// Returns a [models::Response](crate::models::Response) object with response data. /// /// # Arguments /// /// * `request_data`: RequestData object for the request /// * `data`: [`Vec<u8>`] containing data to operate on. fn execute ( request_data : & models :: RequestData , data : Vec < u8 > , ) -> Result < models :: Response , ActiveStorageError > ; } This interface accepts the request data and a byte array containing the storage chunk data in its original byte order. On success, it returns a Response struct which contains a byte array of the response data as well as the data type, shape and a count of non-missing elements in the array. A second NumOperation trait with an execute_t method handles the dynamic dispatch between the runtime data type in the request data and the generic implementation for that type.","title":"The Operation trait"},{"location":"architecture/#operations","text":"Each operation is implemented by a struct that implements the NumOperation trait. For example, the sum operation is implemented by the Sum struct in src/operations.rs . The Sum struct's execute_t method does the following: Zero copy conversion of the byte array to a multi-dimensional ndarray::ArrayView object of the data type, shape and byte order specified in the request data If a selection was specified in the request data, create a sliced ndarray::ArrayView onto the original array view Checks whether the reduction should be performed over all or only a subset of the sliced data's axes Performs a fold over each of the requested axes to calculate the required reduction while ignoring any specified missing data Convert the sum to a byte array and return with the element count The procedure for other operations varies slightly but generally follows the same pattern.","title":"Operations"},{"location":"architecture/#error-handling","text":"The ActiveStorageError enum in src/error.rs describes the various errors that may be returned by the Reductionist API, as well as how to format them for the JSON error response body. Low-level errors are converted to higher-level errors and ultimately wrapped by ActiveStorageError . This is a common pattern in Rust and allows us to describe all of the errors that a function or application may return.","title":"Error handling"},{"location":"architecture/#configuration","text":"Reductionist configuration is implemented in src/cli.rs using the clap library, and accepts command line arguments and environment variables.","title":"Configuration"},{"location":"architecture/#resource-management","text":"Reductionist supports optional restriction of resource usage. This is implemented in src/resource_manager.rs using Tokio Semaphores . This allows Reductionist to limit the quantity of various resources used at any time: S3 connections memory used for numeric data (this is more of a rough guide than a perfect limit) threads used for CPU-bound work","title":"Resource management"},{"location":"architecture/#cpu-bound-work","text":"There is particular friction between the asynchronous and synchronous types of work in the system. Axum and Tokio very efficiently handle the asynchronous aspects such as the HTTP server and S3 object download. The other work such as decompression, filtering and numerical operations are more CPU-bound, and can easily block the Tokio runtime from efficiently handling asynchronous tasks. Two alternative methods were developed to alleviate this issue. The resource manager can limit the number of threads used for CPU-bound work, by default leaving one CPU core free for handling asynchronous tasks. Integration with Rayon , a library that provides a thread pool. Limited benchmarking was done to compare the two approaches, however the first appeared to have lower overhead. The second approach may leave the server more responsive if more CPU-heavy operations are used in future.","title":"CPU-bound work"},{"location":"architecture/#monitoring","text":"Prometheus metrics are implemented in src/metrics.rs and are exposed by the Reductionist API under the /metrics path. These include: incoming requests (counter) outgoing response (counter) response time (histogram)","title":"Monitoring"},{"location":"architecture/#tracing-and-profiling","text":"Reductionist integrates with Jaeger, a distributed tracing platform. Various sections of the request processing pipeline are instrumented with spans, making it easy to visualise the relative durations in the Jaeger UI. Testing with a sum over some CMIP6 temperature data, this showed that in terms of wall clock time, the S3 storage chunk download takes the majority of the time, followed by decompression, byte shuffle, and finally the actual numerical operation. Flame graphs created using flamegraph-rs were useful to visualise which parts of the code consume the most CPU cycles. This was useful to determine where to focus performance improvements, and showed that decompression is the most CPU-heavy task.","title":"Tracing and profiling"},{"location":"contributing/","text":"Contributing \u00b6 Testing of Reductionist \u00b6 Reductionist is tested at various levels. Continuous Integration (CI) \u00b6 GitHub Actions is used for CI for pull requests. It checks that the package builds, and passes various checks, unit and integration tests. Compliance/integration tests \u00b6 The S3 Active Storage compliance test suite should be updated to test any new features added to Reductionist. Code style \u00b6 Rust code style is enforced using cargo fmt . This command will modify the source code to resolve any code style issues. To check for code style issues without making changes, use cargo fmt -- --check . Linting \u00b6 Clippy is used to lint the source code. Use cargo clippy --all-targets -- -D warnings . Unit tests \u00b6 A majority of the application code in Reductionist is unit tested, and any new code should include unit tests where practical. Unit tests in Rust code typically reside in the same file as the module being tested. Unit tests can be run using cargo test . Benchmarks \u00b6 Benchmark tests in the benches directory were created for various modules and used to make performance improvements. These can be run using cargo bench , or a specific benchmark with cargo bench --bench <benchmark name> Pre-commit hook \u00b6 A pre-commit hook is provided in tools/pre-commit that runs formatting, clippy, and unit tests. After cloning this repository, copy it to .git/hooks/pre-commit . Development environment \u00b6 Note For production deployments Reductionist provides an Ansible playbook to easily deploy it and supporting services to one or more hosts. See the deployment guide for details. There are various ways to run the Reductionist server for development purposes. Running in a container \u00b6 The simplest method is to run it in a container using a pre-built image: docker run -it --detach --rm --net = host --name reductionist ghcr.io/stackhpc/reductionist-rs:latest Images are published to GitHub Container Registry when the project is released. The latest tag corresponds to the most recent release, or you can use a specific release e.g. 0.1.0 . This method does not require access to the source code. Building a container image \u00b6 If you need to use unreleased changes, but still want to run in a container, it is possible to build an image. First, clone this repository: git clone https://github.com/stackhpc/reductionist-rs.git cd reductionist-rs make build The image will be tagged as reductionist . The image may be pushed to a registry, or deployed locally. make run Building a Reductionist binary \u00b6 If you prefer not to run the Reductionist server in a container, it will be necessary to build a binary. Building locally may also be preferable during development to take advantage of incremental compilation. Prerequisites \u00b6 This project is written in Rust, and as such requires a Rust toolchain to be installed in order to build it. The Minimum Supported Rust Version (MSRV) is 1.78.0, due to a dependency on the AWS SDK . It may be necessary to use rustup rather than the OS provided Rust toolchain to meet this requirement. See the Rust book for toolchain installation. Build and run Reductionist \u00b6 First, clone this repository: git clone https://github.com/stackhpc/reductionist-rs.git cd reductionist-rs Next, use Cargo to build the package: cargo build --release The active storage server may be run using Cargo: cargo run --release Or installed to the system: cargo install --path . --locked Then run: reductionist Testing \u00b6 For simple testing purposes Minio is a convenient object storage server. Deploy Minio object storage \u00b6 Start a local Minio server which serves the test data: ./scripts/minio-start The Minio server will run in a detached container and may be stopped: ./scripts/minio-stop Note that object data is not preserved when the container is stopped. Upload some test data \u00b6 A script is provided to upload some test data to minio. In a separate terminal, set up the Python virtualenv then upload some sample data: # Create a virtualenv python3 -m venv ./venv # Activate the virtualenv source ./venv/bin/activate # Install dependencies pip install scripts/requirements.txt # Upload some sample data to the running minio server python ./scripts/upload_sample_data.py Compliance test suite \u00b6 Proxy functionality can be tested using the S3 active storage compliance suite . Making requests to active storage endpoints \u00b6 Request authentication is implemented using Basic Auth with the username and password consisting of your S3 Access Key ID and Secret Access Key, respectively. If provided, these credentials are then used internally to authenticate with the upstream S3 source using standard AWS authentication methods . If no basic auth header is provided, an unauthenticated request will be made to S3. A basic Python client is provided in scripts/client.py . First install dependencies in a Python virtual environment: # Create a virtualenv python3 -m venv ./venv # Activate the virtualenv source ./venv/bin/activate # Install dependencies pip install scripts/requirements.txt Then use the client to make a request: venv/bin/python ./scripts/client.py sum --server http://localhost:8080 --source http://localhost:9000 --username minioadmin --password minioadmin --bucket sample-data --object data-uint32.dat --dtype uint32 Documentation \u00b6 Application documentation \u00b6 This documentation is built using MkDocs and hosted on GitHub pages . The configuration file is mkdocs.yml , and documentation Markdown source is in docs/ . GitHub Actions workflows build the documentation in pull requests, and deploy it to GitHub pages on pushes to main . To build and serve the documentation locally at http://127.0.0.1:8000/reductionist-rs : python3 -m venv mkdocs-venv source mkdocs-venv/bin/activate pip install -U pip pip install -r docs-requirements.txt mkdocs serve --strict Source code documentation \u00b6 The source code is documented using rustdoc . Documentation is available on docs.rs . It is also possible to build the documentation locally: cargo doc --no-deps The resulting documentation is available under target/doc , and may be viewed in a web browser using file:///path/to/reductionist/target/doc/reductionist/index.html. Maintenance \u00b6 Updating dependencies \u00b6 Rust package dependencies are managed by Cargo . The [dependencies] section in Cargo.toml defines package dependencies and their version constraints. The [dev-dependencies] section includes additional dependencies for testing. In order to make builds reproducible, Cargo maintains a Cargo.lock file that captures the versions of all package dependencies. This even allows for multiple versions of a package to be used by a single Rust application, although this can lead to incompatibilities at runtime and should be avoided if possible. To update the versions in the Cargo.lock file, run cargo update , then inspect and commit the changes. Updating Minimum Supported Rust Version (MSRV) \u00b6 Rust moves quickly, and it's sensible to keep up with the latest toolchain. With support for multiple installed versions of Rust and the separation provided by containers there is little reason to support old versions of Rust. The AWS SDK for Rust is particularly aggressive in updating its MSRV, and this often drives the MSRV of Reductionist. Updating the Reductionist MSRV requires making changes in a few places. rust-version in Cargo.toml FROM rust:<version> in Dockerfile toolchain in .github/workflows/publish.yml Prerequisites section in docs/contributing.md Updating the MSRV typically requires a few updates to the code to handle changes in the standard library, Clippy rules, etc. Create a release \u00b6 To update the version of Reductionist, set [package] version in Cargo.toml . Create a pull request, approve and merge it. When ready to release, draft a new release in GitHub, creating a new tag matching Reductionist's version, and auto-generating release notes. After the release is published, the publish.yml workflow will be triggered. This workflow publishes the crate to crates.io, and builds then publishes a container image on GHCR.","title":"Contributing"},{"location":"contributing/#contributing","text":"","title":"Contributing"},{"location":"contributing/#testing-of-reductionist","text":"Reductionist is tested at various levels.","title":"Testing of Reductionist"},{"location":"contributing/#continuous-integration-ci","text":"GitHub Actions is used for CI for pull requests. It checks that the package builds, and passes various checks, unit and integration tests.","title":"Continuous Integration (CI)"},{"location":"contributing/#complianceintegration-tests","text":"The S3 Active Storage compliance test suite should be updated to test any new features added to Reductionist.","title":"Compliance/integration tests"},{"location":"contributing/#code-style","text":"Rust code style is enforced using cargo fmt . This command will modify the source code to resolve any code style issues. To check for code style issues without making changes, use cargo fmt -- --check .","title":"Code style"},{"location":"contributing/#linting","text":"Clippy is used to lint the source code. Use cargo clippy --all-targets -- -D warnings .","title":"Linting"},{"location":"contributing/#unit-tests","text":"A majority of the application code in Reductionist is unit tested, and any new code should include unit tests where practical. Unit tests in Rust code typically reside in the same file as the module being tested. Unit tests can be run using cargo test .","title":"Unit tests"},{"location":"contributing/#benchmarks","text":"Benchmark tests in the benches directory were created for various modules and used to make performance improvements. These can be run using cargo bench , or a specific benchmark with cargo bench --bench <benchmark name>","title":"Benchmarks"},{"location":"contributing/#pre-commit-hook","text":"A pre-commit hook is provided in tools/pre-commit that runs formatting, clippy, and unit tests. After cloning this repository, copy it to .git/hooks/pre-commit .","title":"Pre-commit hook"},{"location":"contributing/#development-environment","text":"Note For production deployments Reductionist provides an Ansible playbook to easily deploy it and supporting services to one or more hosts. See the deployment guide for details. There are various ways to run the Reductionist server for development purposes.","title":"Development environment"},{"location":"contributing/#running-in-a-container","text":"The simplest method is to run it in a container using a pre-built image: docker run -it --detach --rm --net = host --name reductionist ghcr.io/stackhpc/reductionist-rs:latest Images are published to GitHub Container Registry when the project is released. The latest tag corresponds to the most recent release, or you can use a specific release e.g. 0.1.0 . This method does not require access to the source code.","title":"Running in a container"},{"location":"contributing/#building-a-container-image","text":"If you need to use unreleased changes, but still want to run in a container, it is possible to build an image. First, clone this repository: git clone https://github.com/stackhpc/reductionist-rs.git cd reductionist-rs make build The image will be tagged as reductionist . The image may be pushed to a registry, or deployed locally. make run","title":"Building a container image"},{"location":"contributing/#building-a-reductionist-binary","text":"If you prefer not to run the Reductionist server in a container, it will be necessary to build a binary. Building locally may also be preferable during development to take advantage of incremental compilation.","title":"Building a Reductionist binary"},{"location":"contributing/#prerequisites","text":"This project is written in Rust, and as such requires a Rust toolchain to be installed in order to build it. The Minimum Supported Rust Version (MSRV) is 1.78.0, due to a dependency on the AWS SDK . It may be necessary to use rustup rather than the OS provided Rust toolchain to meet this requirement. See the Rust book for toolchain installation.","title":"Prerequisites"},{"location":"contributing/#build-and-run-reductionist","text":"First, clone this repository: git clone https://github.com/stackhpc/reductionist-rs.git cd reductionist-rs Next, use Cargo to build the package: cargo build --release The active storage server may be run using Cargo: cargo run --release Or installed to the system: cargo install --path . --locked Then run: reductionist","title":"Build and run Reductionist"},{"location":"contributing/#testing","text":"For simple testing purposes Minio is a convenient object storage server.","title":"Testing"},{"location":"contributing/#deploy-minio-object-storage","text":"Start a local Minio server which serves the test data: ./scripts/minio-start The Minio server will run in a detached container and may be stopped: ./scripts/minio-stop Note that object data is not preserved when the container is stopped.","title":"Deploy Minio object storage"},{"location":"contributing/#upload-some-test-data","text":"A script is provided to upload some test data to minio. In a separate terminal, set up the Python virtualenv then upload some sample data: # Create a virtualenv python3 -m venv ./venv # Activate the virtualenv source ./venv/bin/activate # Install dependencies pip install scripts/requirements.txt # Upload some sample data to the running minio server python ./scripts/upload_sample_data.py","title":"Upload some test data"},{"location":"contributing/#compliance-test-suite","text":"Proxy functionality can be tested using the S3 active storage compliance suite .","title":"Compliance test suite"},{"location":"contributing/#making-requests-to-active-storage-endpoints","text":"Request authentication is implemented using Basic Auth with the username and password consisting of your S3 Access Key ID and Secret Access Key, respectively. If provided, these credentials are then used internally to authenticate with the upstream S3 source using standard AWS authentication methods . If no basic auth header is provided, an unauthenticated request will be made to S3. A basic Python client is provided in scripts/client.py . First install dependencies in a Python virtual environment: # Create a virtualenv python3 -m venv ./venv # Activate the virtualenv source ./venv/bin/activate # Install dependencies pip install scripts/requirements.txt Then use the client to make a request: venv/bin/python ./scripts/client.py sum --server http://localhost:8080 --source http://localhost:9000 --username minioadmin --password minioadmin --bucket sample-data --object data-uint32.dat --dtype uint32","title":"Making requests to active storage endpoints"},{"location":"contributing/#documentation","text":"","title":"Documentation"},{"location":"contributing/#application-documentation","text":"This documentation is built using MkDocs and hosted on GitHub pages . The configuration file is mkdocs.yml , and documentation Markdown source is in docs/ . GitHub Actions workflows build the documentation in pull requests, and deploy it to GitHub pages on pushes to main . To build and serve the documentation locally at http://127.0.0.1:8000/reductionist-rs : python3 -m venv mkdocs-venv source mkdocs-venv/bin/activate pip install -U pip pip install -r docs-requirements.txt mkdocs serve --strict","title":"Application documentation"},{"location":"contributing/#source-code-documentation","text":"The source code is documented using rustdoc . Documentation is available on docs.rs . It is also possible to build the documentation locally: cargo doc --no-deps The resulting documentation is available under target/doc , and may be viewed in a web browser using file:///path/to/reductionist/target/doc/reductionist/index.html.","title":"Source code documentation"},{"location":"contributing/#maintenance","text":"","title":"Maintenance"},{"location":"contributing/#updating-dependencies","text":"Rust package dependencies are managed by Cargo . The [dependencies] section in Cargo.toml defines package dependencies and their version constraints. The [dev-dependencies] section includes additional dependencies for testing. In order to make builds reproducible, Cargo maintains a Cargo.lock file that captures the versions of all package dependencies. This even allows for multiple versions of a package to be used by a single Rust application, although this can lead to incompatibilities at runtime and should be avoided if possible. To update the versions in the Cargo.lock file, run cargo update , then inspect and commit the changes.","title":"Updating dependencies"},{"location":"contributing/#updating-minimum-supported-rust-version-msrv","text":"Rust moves quickly, and it's sensible to keep up with the latest toolchain. With support for multiple installed versions of Rust and the separation provided by containers there is little reason to support old versions of Rust. The AWS SDK for Rust is particularly aggressive in updating its MSRV, and this often drives the MSRV of Reductionist. Updating the Reductionist MSRV requires making changes in a few places. rust-version in Cargo.toml FROM rust:<version> in Dockerfile toolchain in .github/workflows/publish.yml Prerequisites section in docs/contributing.md Updating the MSRV typically requires a few updates to the code to handle changes in the standard library, Clippy rules, etc.","title":"Updating Minimum Supported Rust Version (MSRV)"},{"location":"contributing/#create-a-release","text":"To update the version of Reductionist, set [package] version in Cargo.toml . Create a pull request, approve and merge it. When ready to release, draft a new release in GitHub, creating a new tag matching Reductionist's version, and auto-generating release notes. After the release is published, the publish.yml workflow will be triggered. This workflow publishes the crate to crates.io, and builds then publishes a container image on GHCR.","title":"Create a release"},{"location":"deployment/","text":"Deployment \u00b6 The deployment directory in the Reductionist Git repository contains an Ansible playbook to deploy Reductionist and supporting services to one or more hosts. The Ansible playbook allows for a secure, scale-out deployment of Reductionist, with an HAProxy load balancer proxying requests to any number of Reductionist backend servers. The following services are supported: Podman engine Step CA Certificate Authority (generates certificates for Reductionist) Step CLI (requests and renews certificates) Minio object store (optional, for testing) Prometheus (monitors Reductionist and HAProxy) Jaeger (distributed tracing UI) Reductionist HAProxy (load balancer for Reductionist) Prerequisites \u00b6 The existence of correctly configured hosts is assumed by this playbook. The following host OS distributions have been tested and are supported: CentOS Stream 9 Rocky Linux 9 Ubuntu 24.04 Currently only a single network is supported. Several TCP ports should be accessible on this network. This may require configuration of a firewall on the host (e.g. firewalld, ufw) or security groups in a cloud environment. SSH: 22 Reductionist backend: 8081 Reductionist frontend: 8080 (HAProxy host only) Step CA: 9999 (Step CA host only) Minio: 9000 (Minio host only) Prometheus: 9090 (Prometheus host only) Jaeger: 16686 (Jaeger host only) The Ansible control host (the host from which you will run ansible-playbook ) should be able to resolve the hostnames of the hosts. If names are not provided by DNS, entries may be added to /etc/hosts on the Ansible control host. Issues have been reported when using Ansible with password-protected SSH private keys and SSH agent. It may be desirable to host the Reductionist API on a different address, such as a hostname or public IP exposed on the host running HAProxy. This may be configured using the reductionist_host variable. Configuration \u00b6 An example Ansible inventory file is provided in inventory which defines all groups and maps localhost to them. For a production deployment it is more typical to deploy to one or more remote hosts. The following example inventory places HAProxy, Jaeger, Prometheus and Step CA on reductionist1 , while Reductionist is deployed on reductionist1 and reductionist2 . # Example inventory for deployment to two hosts, reductionist1 and reductionist2. # HAProxy load balancer. # Should contain exactly one host. [haproxy] reductionist1 # Jaeger distributed tracing UI. # Should contain at most one host. [jaeger] reductionist1 # Prometheus monitoring service. # Should contain at most one host. [prometheus] reductionist1 # Reductionist servers. # May contain multiple hosts. [reductionist] reductionist[1 : 2] # Step Certificate Authority (CA). # Should contain exactly one host. [step-ca] reductionist1 # Do not edit. [step:children] reductionist # Do not edit. [podman:children] haproxy jaeger minio prometheus reductionist step-ca Some variables are provided to configure the deployment in the group_vars directory. Reductionist configuration options may be specified using environment variables specified using reductionist_env . Ansible control host setup \u00b6 Whether running Ansible on the same host as the Reductionist server(s) or a separate remote host, some setup is necessary. Ensure Git and Pip are installed: sudo apt -y install git python3-pip # Ubuntu sudo dnf -y install git python3-pip # CentOS Stream or Rocky Linux Clone the Reductionist source code: git clone https://github.com/stackhpc/reductionist-rs Change to the Reductionist source code directory: cd reductionist-rs When working with Pip it's generally best to install packages into a virtual environment, to avoid modifying the system packages. python3 -m venv venv source venv/bin/activate Installation \u00b6 Install Python dependencies: pip install -r deployment/requirements.txt Install Ansible collections: ansible-galaxy collection install -r deployment/requirements.yml Deployment \u00b6 Podman will be used to run containers under the same user account used for ansible deployment. To install requisite system packages some tasks will require sudo privileged access. To run the entire playbook as a non-privileged user prompting for a sudo password: ansible-playbook -i deployment/inventory deployment/site.yml -K To run specific plays the following tags are supported and may be specified via --tags <tag1,tag2> : podman - runs privileged tasks to install the required system packages step-ca step - runs privileged tasks to install the required system packages and Step CA certificate minio prometheus jaeger reductionist haproxy Minimal deployment of Podman and the Reductionist \u00b6 Podman is a prerequisite for running the Reductionist. Podman can run containers as a non-privileged user, however this user must have linger enabled on their account to allow Podman to continue to run after logging out of the user session. To enable linger support for the non-privileged user: sudo loginctl enable-linger <non-privileged user> Alternatively, run the optional podman play to install Podman as a non-privileged user. The following will prompt for the sudo password to escalate privileges only for package installation and for enabling linger for the non-privileged user: ansible-playbook -i deployment/inventory deployment/site.yml --tags podman -K Then to run the reductionist play, again as the non-privileged user: ansible-playbook -i deployment/inventory deployment/site.yml --tags reductionist Podman containers require a manual restart after a system reboot. This requires logging into the host(s) running the Reductionist as the non-privileged user to run: podman restart reductionist Automatic restart on boot can be enabled via systemd , not covered by this documentation. Using SSL/TLS certificates with the Reductionist \u00b6 To enable https connections edit deployment/group_vars/all before deployment as set: REDUCTIONIST_HTTPS: \"true\" Note, this is the default. Create a certs directory under the home directory of the non-privileged deployment user, this will be done automatically and the following files will be added if Step is deployed. If using third party certificates the following files must be added manually using the file names shown: Filename Description certs/key.pem Private key file certs/cert.pem Certificate file including any intermediates Certificates can be added post Reductionist deployment but the Reductionist's container will need to be restarted afterwards. Reductionist Configuration \u00b6 In addition to the certs configuration above the file deployment/group_vars/all covers the following configuration. Ansible Parameter Description reductionist_build_image Whether to locally build the Reductionist container reductionist_src_url Source URL for the Reductionist repository reductionist_src_version Repository branch to use for local builds reductionist_repo_location Where to clone the Reductionist repository reductionist_clone_repo By default the repository cloning overwrites local changes, this disables reductionist_name Name for Reductionist container reductionist_image Container URL if downloading and not building reductionist_tag Container tag reductionist_networks List of container networks reductionist_env Configures the Reductionist environment, see table of environment variables below reductionist_remote_certs_path Path to certificates on the host reductionist_container_certs_path Path to certificates within the container reductionist_remote_cache_path Path to cache on host filesystem reductionist_container_cache_path Path to cache within the container reductionist_volumes Volumes to map from host to container reductionist_host Used when deploying HAProxy to test connectivity to backend Reductionist(s) reductionist_cert_not_after Certificate validity The reductionist_env parameter allows configuration of the environment variables passed to the Reductionist at runtime: Environment Variable Description REDUCTIONIST_HOST The IP address on which to listen on, default \"0.0.0.0\" REDUCTIONIST_PORT Port to listen on REDUCTIONIST_HTTPS Whether to enable https connections REDUCTIONIST_CERT_FILE Path to the certificate file used for https REDUCTIONIST_KEY_FILE Path to the key file used for https REDUCTIONIST_SHUTDOWN_TIMEOUT Maximum time in seconds to wait for operations to complete after receiving the 'ctrl+c' signal REDUCTIONIST_ENABLE_JAEGER Whether to enable sending traces to Jaeger REDUCTIONIST_USE_RAYON Whether to use Rayon for execution of CPU-bound tasks REDUCTIONIST_MEMORY_LIMIT Memory limit in bytes REDUCTIONIST_S3_CONNECTION_LIMIT S3 connection limit REDUCTIONIST_THREAD_LIMIT Thread limit for CPU-bound tasks REDUCTIONIST_USE_CHUNK_CACHE Whether to enable caching of downloaded data objects to disk REDUCTIONIST_CHUNK_CACHE_PATH Absolute filesystem path used for the cache. Defaults to container cache path, see Ansible Parameters above REDUCTIONIST_CHUNK_CACHE_AGE Time in seconds a chunk is kept in the cache REDUCTIONIST_CHUNK_CACHE_PRUNE_INTERVAL Time in seconds between periodic pruning of the cache REDUCTIONIST_CHUNK_CACHE_SIZE_LIMIT Maximum cache size, i.e. \"100GB\" REDUCTIONIST_CHUNK_CACHE_QUEUE_SIZE Tokio MPSC buffer size used to queue downloaded objects between the asynchronous web engine and the synchronous cache REDUCTIONIST_CHUNK_CACHE_BYPASS_AUTH Allow bypassing of S3 authentication when accessing cached data Note, after changing any of the above parameters the Reductionist must be deployed, or redeployed, using the ansible playbook for the change to take effect. The idempotent nature of ansible necessitates that if redeploying then a running Reductionist container must be removed first. Usage \u00b6 Once deployed, the Reductionist API is accessible on port 8080 by HAProxy. The Prometheus UI is accessible on port 9090 on the host running Prometheus. The Jaeger UI is accessible on port 16686 on the host running Jaeger.","title":"Deployment"},{"location":"deployment/#deployment","text":"The deployment directory in the Reductionist Git repository contains an Ansible playbook to deploy Reductionist and supporting services to one or more hosts. The Ansible playbook allows for a secure, scale-out deployment of Reductionist, with an HAProxy load balancer proxying requests to any number of Reductionist backend servers. The following services are supported: Podman engine Step CA Certificate Authority (generates certificates for Reductionist) Step CLI (requests and renews certificates) Minio object store (optional, for testing) Prometheus (monitors Reductionist and HAProxy) Jaeger (distributed tracing UI) Reductionist HAProxy (load balancer for Reductionist)","title":"Deployment"},{"location":"deployment/#prerequisites","text":"The existence of correctly configured hosts is assumed by this playbook. The following host OS distributions have been tested and are supported: CentOS Stream 9 Rocky Linux 9 Ubuntu 24.04 Currently only a single network is supported. Several TCP ports should be accessible on this network. This may require configuration of a firewall on the host (e.g. firewalld, ufw) or security groups in a cloud environment. SSH: 22 Reductionist backend: 8081 Reductionist frontend: 8080 (HAProxy host only) Step CA: 9999 (Step CA host only) Minio: 9000 (Minio host only) Prometheus: 9090 (Prometheus host only) Jaeger: 16686 (Jaeger host only) The Ansible control host (the host from which you will run ansible-playbook ) should be able to resolve the hostnames of the hosts. If names are not provided by DNS, entries may be added to /etc/hosts on the Ansible control host. Issues have been reported when using Ansible with password-protected SSH private keys and SSH agent. It may be desirable to host the Reductionist API on a different address, such as a hostname or public IP exposed on the host running HAProxy. This may be configured using the reductionist_host variable.","title":"Prerequisites"},{"location":"deployment/#configuration","text":"An example Ansible inventory file is provided in inventory which defines all groups and maps localhost to them. For a production deployment it is more typical to deploy to one or more remote hosts. The following example inventory places HAProxy, Jaeger, Prometheus and Step CA on reductionist1 , while Reductionist is deployed on reductionist1 and reductionist2 . # Example inventory for deployment to two hosts, reductionist1 and reductionist2. # HAProxy load balancer. # Should contain exactly one host. [haproxy] reductionist1 # Jaeger distributed tracing UI. # Should contain at most one host. [jaeger] reductionist1 # Prometheus monitoring service. # Should contain at most one host. [prometheus] reductionist1 # Reductionist servers. # May contain multiple hosts. [reductionist] reductionist[1 : 2] # Step Certificate Authority (CA). # Should contain exactly one host. [step-ca] reductionist1 # Do not edit. [step:children] reductionist # Do not edit. [podman:children] haproxy jaeger minio prometheus reductionist step-ca Some variables are provided to configure the deployment in the group_vars directory. Reductionist configuration options may be specified using environment variables specified using reductionist_env .","title":"Configuration"},{"location":"deployment/#ansible-control-host-setup","text":"Whether running Ansible on the same host as the Reductionist server(s) or a separate remote host, some setup is necessary. Ensure Git and Pip are installed: sudo apt -y install git python3-pip # Ubuntu sudo dnf -y install git python3-pip # CentOS Stream or Rocky Linux Clone the Reductionist source code: git clone https://github.com/stackhpc/reductionist-rs Change to the Reductionist source code directory: cd reductionist-rs When working with Pip it's generally best to install packages into a virtual environment, to avoid modifying the system packages. python3 -m venv venv source venv/bin/activate","title":"Ansible control host setup"},{"location":"deployment/#installation","text":"Install Python dependencies: pip install -r deployment/requirements.txt Install Ansible collections: ansible-galaxy collection install -r deployment/requirements.yml","title":"Installation"},{"location":"deployment/#deployment_1","text":"Podman will be used to run containers under the same user account used for ansible deployment. To install requisite system packages some tasks will require sudo privileged access. To run the entire playbook as a non-privileged user prompting for a sudo password: ansible-playbook -i deployment/inventory deployment/site.yml -K To run specific plays the following tags are supported and may be specified via --tags <tag1,tag2> : podman - runs privileged tasks to install the required system packages step-ca step - runs privileged tasks to install the required system packages and Step CA certificate minio prometheus jaeger reductionist haproxy","title":"Deployment"},{"location":"deployment/#minimal-deployment-of-podman-and-the-reductionist","text":"Podman is a prerequisite for running the Reductionist. Podman can run containers as a non-privileged user, however this user must have linger enabled on their account to allow Podman to continue to run after logging out of the user session. To enable linger support for the non-privileged user: sudo loginctl enable-linger <non-privileged user> Alternatively, run the optional podman play to install Podman as a non-privileged user. The following will prompt for the sudo password to escalate privileges only for package installation and for enabling linger for the non-privileged user: ansible-playbook -i deployment/inventory deployment/site.yml --tags podman -K Then to run the reductionist play, again as the non-privileged user: ansible-playbook -i deployment/inventory deployment/site.yml --tags reductionist Podman containers require a manual restart after a system reboot. This requires logging into the host(s) running the Reductionist as the non-privileged user to run: podman restart reductionist Automatic restart on boot can be enabled via systemd , not covered by this documentation.","title":"Minimal deployment of Podman and the Reductionist"},{"location":"deployment/#using-ssltls-certificates-with-the-reductionist","text":"To enable https connections edit deployment/group_vars/all before deployment as set: REDUCTIONIST_HTTPS: \"true\" Note, this is the default. Create a certs directory under the home directory of the non-privileged deployment user, this will be done automatically and the following files will be added if Step is deployed. If using third party certificates the following files must be added manually using the file names shown: Filename Description certs/key.pem Private key file certs/cert.pem Certificate file including any intermediates Certificates can be added post Reductionist deployment but the Reductionist's container will need to be restarted afterwards.","title":"Using SSL/TLS certificates with the Reductionist"},{"location":"deployment/#reductionist-configuration","text":"In addition to the certs configuration above the file deployment/group_vars/all covers the following configuration. Ansible Parameter Description reductionist_build_image Whether to locally build the Reductionist container reductionist_src_url Source URL for the Reductionist repository reductionist_src_version Repository branch to use for local builds reductionist_repo_location Where to clone the Reductionist repository reductionist_clone_repo By default the repository cloning overwrites local changes, this disables reductionist_name Name for Reductionist container reductionist_image Container URL if downloading and not building reductionist_tag Container tag reductionist_networks List of container networks reductionist_env Configures the Reductionist environment, see table of environment variables below reductionist_remote_certs_path Path to certificates on the host reductionist_container_certs_path Path to certificates within the container reductionist_remote_cache_path Path to cache on host filesystem reductionist_container_cache_path Path to cache within the container reductionist_volumes Volumes to map from host to container reductionist_host Used when deploying HAProxy to test connectivity to backend Reductionist(s) reductionist_cert_not_after Certificate validity The reductionist_env parameter allows configuration of the environment variables passed to the Reductionist at runtime: Environment Variable Description REDUCTIONIST_HOST The IP address on which to listen on, default \"0.0.0.0\" REDUCTIONIST_PORT Port to listen on REDUCTIONIST_HTTPS Whether to enable https connections REDUCTIONIST_CERT_FILE Path to the certificate file used for https REDUCTIONIST_KEY_FILE Path to the key file used for https REDUCTIONIST_SHUTDOWN_TIMEOUT Maximum time in seconds to wait for operations to complete after receiving the 'ctrl+c' signal REDUCTIONIST_ENABLE_JAEGER Whether to enable sending traces to Jaeger REDUCTIONIST_USE_RAYON Whether to use Rayon for execution of CPU-bound tasks REDUCTIONIST_MEMORY_LIMIT Memory limit in bytes REDUCTIONIST_S3_CONNECTION_LIMIT S3 connection limit REDUCTIONIST_THREAD_LIMIT Thread limit for CPU-bound tasks REDUCTIONIST_USE_CHUNK_CACHE Whether to enable caching of downloaded data objects to disk REDUCTIONIST_CHUNK_CACHE_PATH Absolute filesystem path used for the cache. Defaults to container cache path, see Ansible Parameters above REDUCTIONIST_CHUNK_CACHE_AGE Time in seconds a chunk is kept in the cache REDUCTIONIST_CHUNK_CACHE_PRUNE_INTERVAL Time in seconds between periodic pruning of the cache REDUCTIONIST_CHUNK_CACHE_SIZE_LIMIT Maximum cache size, i.e. \"100GB\" REDUCTIONIST_CHUNK_CACHE_QUEUE_SIZE Tokio MPSC buffer size used to queue downloaded objects between the asynchronous web engine and the synchronous cache REDUCTIONIST_CHUNK_CACHE_BYPASS_AUTH Allow bypassing of S3 authentication when accessing cached data Note, after changing any of the above parameters the Reductionist must be deployed, or redeployed, using the ansible playbook for the change to take effect. The idempotent nature of ansible necessitates that if redeploying then a running Reductionist container must be removed first.","title":"Reductionist Configuration"},{"location":"deployment/#usage","text":"Once deployed, the Reductionist API is accessible on port 8080 by HAProxy. The Prometheus UI is accessible on port 9090 on the host running Prometheus. The Jaeger UI is accessible on port 16686 on the host running Jaeger.","title":"Usage"},{"location":"pyactivestorage/","text":"Integration of Reductionist and PyActiveStorage \u00b6 Reductionist has been integrated with the PyActiveStorage library, and PyActiveStorage acts as a client of the Reductionist server. PyActiveStorage currently works with data in netCDF4 format, and is able to perform reductions on a variable within such a dataset. Numerical operations are performed on individual storage chunks, with the results later aggregated. The original POSIX/NumPy storage chunk reduction in PyActiveStorage is implemented in a reduce_chunk Python function in activestorage/storage.py , and this interface was used as the basis for the integration of Reductionist. The following code snippet shows the reduce_chunk function signature. def reduce_chunk ( rfile , offset , size , compression , filters , missing , dtype , shape , order , chunk_selection , method = None ): \"\"\" We do our own read of chunks and decoding etc rfile - the actual file with the data offset, size - where and what we want ... compression - optional `numcodecs.abc.Codec` compression codec filters - optional list of `numcodecs.abc.Codec` filter codecs dtype - likely float32 in most cases. shape - will be a tuple, something like (3,3,1), this is the dimensionality of the chunk itself order - typically 'C' for c-type ordering chunk_selection - python slice tuples for each dimension, e.g. (slice(0, 2, 1), slice(1, 3, 1), slice(0, 1, 1)) this defines the part of the chunk which is to be obtained or operated upon. method - computation desired (in this Python version it's an actual method, in storage implementations we'll change to controlled vocabulary) \"\"\" For Reductionist, the reduce_chunk function signature in activestorage/reductionist.py is similar, but replaces the local file path with a requests.Session object, the Reductionist server URL, S3-compatible object store URL, and the bucket and object containing the data. def reduce_chunk ( session , server , source , bucket , object , offset , size , compression , filters , missing , dtype , shape , order , chunk_selection , operation ): \"\"\"Perform a reduction on a chunk using Reductionist. :param session: requests.Session object :param server: Reductionist server URL :param source: S3 URL :param bucket: S3 bucket :param object: S3 object :param offset: offset of data in object :param size: size of data in object :param compression: optional `numcodecs.abc.Codec` compression codec :param filters: optional list of `numcodecs.abc.Codec` filter codecs :param missing: optional 4-tuple describing missing data :param dtype: numpy data type :param shape: will be a tuple, something like (3,3,1), this is the dimensionality of the chunk itself :param order: typically 'C' for c-type ordering :param chunk_selection: N-tuple where N is the length of `shape`, and each item is an integer or slice. e.g. (slice(0, 2, 1), slice(1, 3, 1), slice(0, 1, 1)) this defines the part of the chunk which is to be obtained or operated upon. :param operation: name of operation to perform :returns: the reduced data as a numpy array or scalar :raises ReductionistError: if the request to Reductionist fails \"\"\" Within the reduce_chunk implementation for Reductionist, the following steps are taken: build Reductionist API request data build Reductionist API URL perform an HTTP(S) POST request to Reductionist on success, return a NumPy array containing the data in the response payload, with data type, shape and count determined by response headers on failure, raise a ReductionistError with the response status code and JSON encoded error response The use of a requests.Session object allows for TCP connection pooling, reducing connection overhead when multiple requests are made within a short timeframe. It should be possible to provide a unified interface to storage systems by abstracting away the details of the storage system and data source, but this has not yet been done. Other changes to the main activestorage.Active class were necessary for integration of Reductionist. These include: Support for reading netCDF metadata from files stored in S3 using the s3fs and h5netcdf libraries Configuration options in activestorage/config.py to specify the Reductionist API URL, S3-compatible object store URL, S3 access key, secret key and bucket Constructor storage_type argument for activestorage.Active to specify the storage backend Use of a thread pool to execute storage chunk reductions in parallel Unit tests to cover new and modified code Integration test changes to allow running against a POSIX or S3 storage backend","title":"PyActiveStorage Integration"},{"location":"pyactivestorage/#integration-of-reductionist-and-pyactivestorage","text":"Reductionist has been integrated with the PyActiveStorage library, and PyActiveStorage acts as a client of the Reductionist server. PyActiveStorage currently works with data in netCDF4 format, and is able to perform reductions on a variable within such a dataset. Numerical operations are performed on individual storage chunks, with the results later aggregated. The original POSIX/NumPy storage chunk reduction in PyActiveStorage is implemented in a reduce_chunk Python function in activestorage/storage.py , and this interface was used as the basis for the integration of Reductionist. The following code snippet shows the reduce_chunk function signature. def reduce_chunk ( rfile , offset , size , compression , filters , missing , dtype , shape , order , chunk_selection , method = None ): \"\"\" We do our own read of chunks and decoding etc rfile - the actual file with the data offset, size - where and what we want ... compression - optional `numcodecs.abc.Codec` compression codec filters - optional list of `numcodecs.abc.Codec` filter codecs dtype - likely float32 in most cases. shape - will be a tuple, something like (3,3,1), this is the dimensionality of the chunk itself order - typically 'C' for c-type ordering chunk_selection - python slice tuples for each dimension, e.g. (slice(0, 2, 1), slice(1, 3, 1), slice(0, 1, 1)) this defines the part of the chunk which is to be obtained or operated upon. method - computation desired (in this Python version it's an actual method, in storage implementations we'll change to controlled vocabulary) \"\"\" For Reductionist, the reduce_chunk function signature in activestorage/reductionist.py is similar, but replaces the local file path with a requests.Session object, the Reductionist server URL, S3-compatible object store URL, and the bucket and object containing the data. def reduce_chunk ( session , server , source , bucket , object , offset , size , compression , filters , missing , dtype , shape , order , chunk_selection , operation ): \"\"\"Perform a reduction on a chunk using Reductionist. :param session: requests.Session object :param server: Reductionist server URL :param source: S3 URL :param bucket: S3 bucket :param object: S3 object :param offset: offset of data in object :param size: size of data in object :param compression: optional `numcodecs.abc.Codec` compression codec :param filters: optional list of `numcodecs.abc.Codec` filter codecs :param missing: optional 4-tuple describing missing data :param dtype: numpy data type :param shape: will be a tuple, something like (3,3,1), this is the dimensionality of the chunk itself :param order: typically 'C' for c-type ordering :param chunk_selection: N-tuple where N is the length of `shape`, and each item is an integer or slice. e.g. (slice(0, 2, 1), slice(1, 3, 1), slice(0, 1, 1)) this defines the part of the chunk which is to be obtained or operated upon. :param operation: name of operation to perform :returns: the reduced data as a numpy array or scalar :raises ReductionistError: if the request to Reductionist fails \"\"\" Within the reduce_chunk implementation for Reductionist, the following steps are taken: build Reductionist API request data build Reductionist API URL perform an HTTP(S) POST request to Reductionist on success, return a NumPy array containing the data in the response payload, with data type, shape and count determined by response headers on failure, raise a ReductionistError with the response status code and JSON encoded error response The use of a requests.Session object allows for TCP connection pooling, reducing connection overhead when multiple requests are made within a short timeframe. It should be possible to provide a unified interface to storage systems by abstracting away the details of the storage system and data source, but this has not yet been done. Other changes to the main activestorage.Active class were necessary for integration of Reductionist. These include: Support for reading netCDF metadata from files stored in S3 using the s3fs and h5netcdf libraries Configuration options in activestorage/config.py to specify the Reductionist API URL, S3-compatible object store URL, S3 access key, secret key and bucket Constructor storage_type argument for activestorage.Active to specify the storage backend Use of a thread pool to execute storage chunk reductions in parallel Unit tests to cover new and modified code Integration test changes to allow running against a POSIX or S3 storage backend","title":"Integration of Reductionist and PyActiveStorage"}]}